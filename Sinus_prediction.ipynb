{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation \n",
    "\n",
    "Arguments:\n",
    "\n",
    "-mu: mean value of the noise;\n",
    "\n",
    "-sigma: standard deviation;\n",
    "\n",
    "Function: $sin(2\\pi x)+N(0,0.3)$\n",
    "\n",
    "Outputs:\n",
    "\n",
    "-x: np.array of size (1000,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(mu=0,sigma=0.3):\n",
    "    t=np.arange(0,10,0.01)\n",
    "    s = np.random.normal(mu, sigma, 1000)\n",
    "    x=(np.sin(2*np.pi*t)+s)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Arguments: \n",
    "\n",
    "-time_wind: length of the sequence of the historical data;\n",
    "\n",
    "-output: how many steps ahead prediction is going to be performed;\n",
    "\n",
    "-nb_var: number of variables of the data set;\n",
    "\n",
    "Outputs:\n",
    "\n",
    "-x_batches: input for the training set. Size: (899, 2, 1);\n",
    "\n",
    "-y_batches: output for the training set. Size: (899, 1);\n",
    "\n",
    "-x_test: inputs for the test set; Size: (96, 2, 1);\n",
    "\n",
    "-y_test: output for the test set; Size: (96, 1);\n",
    "\n",
    "! Note, the size of the first dimension will vary depending on the size of the time_wind. The second dimension refers to the time_wind size, thisrd dimension: number of variables;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preparation(data,time_wind=2,output=1,nb_var=1,batch_size=1):\n",
    "    data_train=data[:901]\n",
    "    x_train=[]\n",
    "    for i in range((len(data_train)-1)-(time_wind-1)):\n",
    "        x_train.append(data_train[i:(i+time_wind)])\n",
    "    x_train=np.array(x_train)\n",
    "    x_batches=(x_train[:(len(x_train)-(len(x_train)%batch_size))]).reshape(-1,time_wind,nb_var)\n",
    "    y_train=data_train[time_wind:len(data_train)]\n",
    "    y_batches=(y_train[:(len(y_train)-(len(y_train)%batch_size))]).reshape(-1,output)\n",
    "    data_test=data[902:]\n",
    "    x_t=[]\n",
    "    for i in range((len(data_test)-1)-(time_wind-1)):\n",
    "        x_t.append(data_test[i:(i+time_wind)])\n",
    "    x_t=np.array(x_t)\n",
    "    x_test=(x_t[:(len(x_t)-(len(x_t)%batch_size))]).reshape(-1,time_wind,nb_var)\n",
    "    y_t=data_test[time_wind:len(data_test)]\n",
    "    y_test=(y_t[:(len(y_t)-(len(y_t)%batch_size))]).reshape(-1,output)\n",
    "    return x_batches, y_batches, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow computation graph\n",
    "\n",
    "Arguments:\n",
    "\n",
    "-epochs: how many times the network is going to be trained;\n",
    "\n",
    "-time_wind: the length of the historical sequence;\n",
    "\n",
    "-hidden: number of hidden layers;\n",
    "\n",
    "-nb_var: amount of input variables;\n",
    "\n",
    "-output: how many steps ahead prediction is going to be performed;\n",
    "\n",
    "-lr: learning rate.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "-mse_test: the mean squared error for the final prediction vector;\n",
    "\n",
    "-y_pred: the vector of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(epochs,time_wind,hidden=10,nb_var=1,output=1,lr=0.01):\n",
    "\n",
    "    from tensorflow.contrib import rnn\n",
    "    tf.reset_default_graph()\n",
    "    X=tf.placeholder(tf.float32,[None,time_wind,nb_var])\n",
    "    Y=tf.placeholder(tf.float32,[None,output])\n",
    " \n",
    "    tf.set_random_seed(1)\n",
    "    x=tf.unstack(X,time_wind,1) # we use static RNN, so the dimension of the input should be reduced. \n",
    "    \n",
    "    #After unstack dimension: we receive \"time_wind\" tensors of the shape (length, nb_var)\n",
    "    \n",
    "    basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden) # here the first set of weights and biases is defined\n",
    "    rnn_output, states=rnn.static_rnn(basic_cell,x,dtype=tf.float32)\n",
    "\n",
    "    stacked_rnn_output=tf.reshape(rnn_output[-1],[-1,hidden])\n",
    "    stacked_outputs=tf.layers.dense(stacked_rnn_output,output) # the output layer. (+additional set of weights and biases)\n",
    "    loss=tf.reduce_mean(tf.squared_difference(stacked_outputs, Y))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for ep in range(epochs):\n",
    "            sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "            if ep % 100 == 0:\n",
    "                mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "                print(ep,\"\\tMSE:\",mse)\n",
    "        y_pred=sess.run(stacked_outputs,feed_dict={X:x_test})\n",
    "        mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "        print(mse)\n",
    "        print(mse_test)\n",
    "    sess.close() \n",
    "    return mse_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisastion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualisation():\n",
    "    plt.plot(y_test,label='Actual')\n",
    "    plt.plot(y_pred,label='Forecast')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    return plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse=[]\n",
    "for i in range (10):\n",
    "    np.random.seed(i)\n",
    "    x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),1)\n",
    "    mse.append(metrics.mean_squared_error(x_batches[:,0],np.full(len(x_batches),np.mean(x_batches))))\n",
    "print(\"Mean of MSE for the naive model:{}\".format(np.mean(mse)))\n",
    "print(\"Variance of MSE for the naive model:{}\".format(np.var(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse=[]\n",
    "for i in range (10):\n",
    "    np.random.seed(i)\n",
    "    x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),1)\n",
    "    x_t1=y_test[:96]\n",
    "    x_t=y_test[1:]\n",
    "    mse.append(metrics.mean_squared_error(x_t,x_t1))\n",
    "print(\"Mean of MSE for the naive model:{}\".format(np.mean(mse)))\n",
    "print(\"Variance of MSE for the naive model:{}\".format(np.var(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Time_wind\n",
    "The aim of the first  experiment is to define the optimal size of the time_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_mean=[]\n",
    "mse_var=[]\n",
    "for j in range(1,9,1):\n",
    "    mse=[]\n",
    "    for i in range (10):\n",
    "        np.random.seed(i)\n",
    "        x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),j)\n",
    "        mse_test,y_pred=run_graph(1000,j,hidden=1,lr=0.1)\n",
    "        mse.append(mse_test)\n",
    "    mse_mean.append(np.mean(mse))\n",
    "    mse_var.append(np.var(mse))\n",
    "    print(\"Mean of MSE for the model with time_wind={} equals{}\".format(j, np.mean(mse)))\n",
    "    print(\"Variance of MSE for the model with time_wind={} equals{}\".format(j, np.var(mse)))\n",
    "visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: the highest accuracy was obtained with time_wind=6. Then MSE=0.10424. The accuracy of the basic model with time_wind=1 is equal to 0.133629, what is better the the results of the naive model.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden layers\n",
    "The aim of the second experiment is to discover wether the increment of the number of hidden layers will improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_mean=[]\n",
    "mse_var=[]\n",
    "for j in range(1,11,1):\n",
    "    mse=[]\n",
    "    for i in range (10):\n",
    "        np.random.seed(i)\n",
    "        x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),6)\n",
    "        mse_test,y_pred=run_graph(1000,6,hidden=j,lr=0.1)\n",
    "        mse.append(mse_test)\n",
    "    mse_mean.append(np.mean(mse))\n",
    "    mse_var.append(np.var(mse))\n",
    "    print(\"Mean of MSE for the model with hidden={} equals{}\".format(j, np.mean(mse)))\n",
    "    print(\"Variance of MSE for the model with hidden={} equals{}\".format(j, np.var(mse)))\n",
    "visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: additional hidden layers make the quality of predictions worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation of the training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_mean=[]\n",
    "mse_var=[]\n",
    "for j in range(1000,11000,1000):\n",
    "    mse=[]\n",
    "    for i in range (10):\n",
    "        np.random.seed(i)\n",
    "        x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),6)\n",
    "        mse_test,y_pred=run_graph(j,6,hidden=1,lr=0.1)\n",
    "        mse.append(mse_test)\n",
    "    mse_mean.append(np.mean(mse))\n",
    "    mse_var.append(np.var(mse))\n",
    "    print(\"Mean of MSE for the model with epochs={} equals {}\".format(j, np.mean(mse)))\n",
    "    print(\"Variance of MSE for the model with epochs={} equals {}\".format(j, np.var(mse)))\n",
    "visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: With the learning rate 0.1 the lowest MSE is obtained with number of training epochs=5000. MSE=0.104167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_mean=[]\n",
    "mse_var=[]\n",
    "for j in range(1000,11000,1000):\n",
    "    mse=[]\n",
    "    for i in range (10):\n",
    "        np.random.seed(i)\n",
    "        x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),6)\n",
    "        mse_test,y_pred=run_graph(j,6,hidden=1,lr=0.01)\n",
    "        mse.append(mse_test)\n",
    "    mse_mean.append(np.mean(mse))\n",
    "    mse_var.append(np.var(mse))\n",
    "    print(\"Mean of MSE for the model with epochs={} equals {}\".format(j, np.mean(mse)))\n",
    "    print(\"Variance of MSE for the model with epochs={} equals {}\".format(j, np.var(mse)))\n",
    "visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_mean[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: With the learning rate of 0.01, the highest ouput is obtained with number of epochs=5000. MSE=0.10420541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate: 0.001\n",
    "As the learning rate is decreased, it makes sense to start the experiment with epochs=3000, and finishes with epochs=16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_mean=[]\n",
    "mse_var=[]\n",
    "for j in range(3000,17000,1000):\n",
    "    mse=[]\n",
    "    for i in range (10):\n",
    "        np.random.seed(i)\n",
    "        x_batches, y_batches, x_test, y_test=data_preparation(generate_data(),6)\n",
    "        mse_test,y_pred=run_graph(j,6,hidden=1,lr=0.001)\n",
    "        mse.append(mse_test)\n",
    "    mse_mean.append(np.mean(mse))\n",
    "    mse_var.append(np.var(mse))\n",
    "    print(\"Mean of MSE for the model with epochs={} equals {}\".format(j, np.mean(mse)))\n",
    "    print(\"Variance of MSE for the model with epochs={} equals {}\".format(j, np.var(mse)))\n",
    "visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mse_mean[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: with the lr=0.001, the highest accuracy was obtained with the epochs=12000. MSE=0.10420661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
